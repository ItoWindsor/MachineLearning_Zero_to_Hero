{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Linear Regression      \n",
    "#### Personal Notes"
   ],
   "id": "44818cfc0ffbb79e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Introduction ",
   "id": "ca2e9e91166a90fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Suppose that we have $ X = X_1, X_2, ..., X_p$ $\\in \\mathbb{R}^{p}$ with $p \\in \\mathbb{N}$ and we wish to predict $Y \\in \\mathbb{R}$ and the joint distribution between X and Y is $\\mathbb{P}_{R} \\left( X,Y \\right)$.      \n",
    "Therefore, we seek a function $f$ such that :     \n",
    "$$\n",
    "Y = f \\left( X \\right) \n",
    "$$\n",
    "To determine if a function $f$ is a good candidate, we need to estimate the error and thus need a \"Loss Function\" $L$. The most common is the \"squared error loss\", i.e :    \n",
    "$$\n",
    "L : y,z \\mapsto (y - z)^2\n",
    "$$\n",
    "Which leads us to the following criterion : \n",
    "$$\n",
    "EPE \\left( f \\right) = \\mathbb{E} \\left[ L \\left( Y ,f \\left( X \\right) \\right) \\right] = \\mathbb{E} \\left[ \\left( Y - f \\left( X \\right) \\right)^2 \\right]  \n",
    "$$\n",
    "In This particular setting, the best candidate for $f$ is :  \n",
    "$$\n",
    "f : x \\mapsto \\mathbb{E} \\left[ Y | X = x \\right] \n",
    "$$ \n",
    "This function f is called the **regression function**."
   ],
   "id": "10b5c6d2fb13fd91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## I - Principle of the model",
   "id": "7222ad183984e7d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Determining $f$ is in reality hard as we only have access to a finite number of data points $\\mathcal{T}_{training} = (x_i,y_i)_{1 \\leq N}$. Indeed, this lead to issue as by example if we used the RSS (**R**esidual **S**um of **S**quares) as a criterion to determine if a function is good enough, i.e : \n",
    "$$\n",
    "RSS \\left( f \\right) = \\sum_{i=1}^{N} \\left( y_i - f \\left( x_i \\right) \\right) \n",
    "$$\n",
    "What we clearly understand is that each function going exactly through the points at $y_i$ would be a perfect fit. Therefore, to solve our problem we are going to make the following assumptions : \n",
    "- The relationship between X and Y is defined as the following : \n",
    "$$\n",
    "Y = f \\left( X \\right) + \\varepsilon \\quad \\varepsilon \\sim \\mathcal{N} \\left( 0, \\sigma^2 \\right) \\quad \\text{and} \\quad \\varepsilon \\perp\\!\\!\\!\\!\\perp X\n",
    "$$  \n",
    "This assumption means that the conditionnal distribution $\\mathbb{P}_{R} \\left( X,Y \\right)$ depends on $X$ only through $f \\left( x \\right)$ \n",
    "- The underlying structure of the data is the following : \n",
    "$$\n",
    "f(X) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j X_j\n",
    "$$\n",
    "Where the coefficients $\\left( \\beta \\right)_{1 \\leq i \\leq p}$ are found by solving the following optimization problem : \n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\beta^{\\star} &= argmin_{\\beta \\in \\mathbb{R}^{p+1}} RSS \\left( \\beta \\right) \\\\\n",
    "    &= argmin_{\\beta \\in \\mathbb{R}^{p+1}} \\left( \\sum_{i=1}^{N} \\left( y_i - f \\left( x_i \\right) \\right)^2  \\right)\\\\\n",
    "    &= argmin_{\\beta \\in \\mathbb{R}^{p+1}} \\left( \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_p \\right)^2 \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "What we can remark is that by defining the structure of our model, we went from a class of function to a class of parameters. Therefore, now identifying the good function $f$ is the same as identifying the coefficients $\\beta_i$. Therefore, in the following we will use $f$ and $\\beta$ indistinctively."
   ],
   "id": "3056035a59112b9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## II - Penalization methods ",
   "id": "acbbd203e037fe93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Earlier we used the RSS criterion to determine if a function f fitted or not. However, the issue is that we would have a good fit on the training set but poor results on the test set. Therefore, we understand that our criterion is not the most adapted, we need to somehow modify it. One of the way is to add a penalty term, i.e : we don't use **RSS** but **PRSS** (**P**enalised **R**esidual **S**um of **S**quares) : \n",
    "$$\n",
    "PRSS \\left( f , \\lambda \\right) = RSS \\left( f \\right) + \\lambda J \\left( f \\right) \n",
    "$$ \n",
    "Where J denotes the penalization.         \n",
    "In fact this formulation is the Lagrangian one. A penalization means that we put some constraints. In our case our objective function is the RSS, and we selected a $t \\in \\mathbb{R}^{+}$ such that our constrained problem is : \n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\inf \\left( \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_p \\right)^2 \\right) \\\\\n",
    "    &\\text{s.t : } J \\left( \\beta \\right) \\leq t \n",
    "\\end{align*}\n",
    "$$"
   ],
   "id": "5fde42436a5b1f4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Lasso Regression ",
   "id": "d9d1e6a86da731d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The ***LASSO** ( **L**east **A**bsolute **S**hrinkage and **S**election **O**perator ) Regression is a regularization method where the function J is the $L_1$ norm. In our case we apply it to the linear regression thus on the coefficients $ \\left( \\beta_i \\right)_{1 \\leq i \\leq p}$ which gives : \n",
    "$$\n",
    "PRSS \\left( \\beta, \\lambda \\right) = RSS \\left( \\beta \\right) + \\lambda J || \\beta ||_{2}^{2}\n",
    "$$"
   ],
   "id": "d694c4480804e5df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ridge Regression ",
   "id": "73420ebbce81bba6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bridge Regression ",
   "id": "4ea9a3245d52db51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Elastic-Net",
   "id": "c0c666b60df250cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sources",
   "id": "5c345842cdb3bd41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[1] Hastie T., Tibshirani R., Friedman J., \"Elements of Statistical Learning\", 2009",
   "id": "4d8c66e47991906e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
